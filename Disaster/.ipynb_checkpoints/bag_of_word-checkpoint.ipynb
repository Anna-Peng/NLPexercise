{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "import spacy.gold\n",
    "from sklearn.base import TransformerMixin,BaseEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import string\n",
    "from spacy.matcher import Matcher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy to preprocess text into lemmatised tokens\n",
    "### Sklearn pipeline models:\n",
    "    1) Countvectoriser\n",
    "    2) Tfidf Vectoriser\n",
    "    3) Random Forest\n",
    "    4) Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install spacy && python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= pd.read_csv('train.csv')\n",
    "df2 = pd.read_csv('test.csv')\n",
    "\n",
    "boot = False # resample the data to 10000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all the words into the same columns\n",
    "def fill_words(data):\n",
    "    keyword_filled = data.keyword.fillna('')\n",
    "    location_filled = data.location.fillna('')\n",
    "    data['all_words'] = data.text + ' ' + keyword_filled + ' ' + location_filled\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = fill_words(df1)\n",
    "df_test = fill_words(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_train, test_size=0.33, random_state=42)\n",
    "if boot == True: train = train.sample(n=10000, replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613 5100 2513\n"
     ]
    }
   ],
   "source": [
    "print(len(df1), len(train), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tokenizer to clean words. This will be used in pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate nlp object and load all the things we want to get rid of\n",
    "def init_parser():\n",
    "    # Load English tokenizer, tagger, parser, NER and word vectors\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    # Create our list of punctuation marks\n",
    "    punctuations = string.punctuation\n",
    "    SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-\", \"...\", \"”\", \"”\", '']\n",
    "    # Create our list of stopwords\n",
    "    stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = [{\"TEXT\": {\"REGEX\": \"^https?:\\/\\/.*[\\r\\n]*\"}}]\n",
    "    matcher.add(\"URL\", [pattern])\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = nlp(sentence)\n",
    "    matches = matcher(mytokens)\n",
    "    \n",
    "    \n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in SYMBOLS ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "matches = matcher(df_train['all_words'][100][:])\n",
    "for match_id, start, end in matcher:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = df_train['URL'][start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deed', 'reason', 'earthquake', 'allah', 'forgive']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tokenizer(df_train['all_words'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These functions are for the pipeline for ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dense transformer\n",
    "class ToDenseTransformer(BaseEstimator,TransformerMixin):\n",
    "    # define the transform operation\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    # no paramter to learn this case\n",
    "    # fit just returns an unchanged object\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printNMostInformative(vectorizer, clf, N):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    topClass1 = coefs_with_fns[:N]\n",
    "    topClass2 = coefs_with_fns[:-(N + 1):-1]\n",
    "    print(\"Not Disaster Best Words: \")\n",
    "    for feat in topClass1:\n",
    "        print(feat)\n",
    "    print(\"Disaster Best words: \")\n",
    "    for feat in topClass2:\n",
    "        print(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Training and Verification Data\n",
    "X_train = train['all_words'].tolist()\n",
    "Y_train = train['target'].tolist()\n",
    "X_test = test['all_words'].tolist()\n",
    "Y_test = test['target'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words vectorizer\n",
    "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer)\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer),\n",
    "    ('normal', ToDenseTransformer()),\n",
    "    (\"clf\", clf)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: for countvectorizer model is: 0.7906884202148826\n",
      "Not Disaster Best Words: \n",
      "(-1.2151818683151665, 'wrong')\n",
      "(-1.0494782685218167, 'blight')\n",
      "(-1.0325147227084714, 'bitch')\n",
      "(-1.0168009728034149, 'attempt')\n",
      "(-0.9982631376880118, 'nowplaye')\n",
      "(-0.9872856067944742, 'aftershock')\n",
      "(-0.9686608518945959, 'ebay')\n",
      "(-0.9675184921458493, 'career')\n",
      "(-0.9604457407355692, 'orlando')\n",
      "(-0.9543022626632774, 'wedding')\n",
      "Disaster Best words: \n",
      "(2.4350178931890007, 'hiroshima')\n",
      "(1.9223857571433098, 'wildfire')\n",
      "(1.876228846486541, 'earthquake')\n",
      "(1.6231019375287181, 'suicide')\n",
      "(1.5046531711992408, 'typhoon')\n",
      "(1.488764651119117, 'debris')\n",
      "(1.4853987377086897, 'migrant')\n",
      "(1.4822323604861207, 'derailment')\n",
      "(1.456701812650832, 'spill')\n",
      "(1.4394601376531029, 'village')\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "pipe.fit(X_train, Y_train)\n",
    "# test\n",
    "preds = pipe.predict(X_test)\n",
    "accu = preds == Y_test\n",
    "print(\"accuracy: for countvectorizer model is:\", accu.mean())\n",
    "printNMostInformative(vectorizer, clf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: for Tfidf model is: 0.7811380819737366\n",
      "Not Disaster Best Words: \n",
      "(-2.019729482871739, 'scream')\n",
      "(-2.0067750517357483, 'panic')\n",
      "(-1.7876223097711863, 'love')\n",
      "(-1.7220919293122998, 'ruin')\n",
      "(-1.684725447747696, 'wreck')\n",
      "(-1.6298055933778022, 'let')\n",
      "(-1.609355192088619, 'blaze')\n",
      "(-1.5697244619467763, 'aftershock')\n",
      "(-1.559562139568242, 'twister')\n",
      "(-1.5490903985222768, 'new')\n",
      "Disaster Best words: \n",
      "(3.1825948404479796, 'hiroshima')\n",
      "(2.7287800772859554, 'kill')\n",
      "(2.6459608240137342, 'wildfire')\n",
      "(2.528786274557909, 'earthquake')\n",
      "(2.4277111843578085, 'suicide')\n",
      "(2.3556583493855383, 'bombing')\n",
      "(2.3363114607398563, 'typhoon')\n",
      "(2.189955219273723, 'train')\n",
      "(2.1766169842646947, 'debris')\n",
      "(2.091640961388825, 'fire')\n"
     ]
    }
   ],
   "source": [
    "# Tfidf vectorizer\n",
    "pipe_Tfidf = Pipeline(\n",
    "    [(\"vect\", vectorizer),\n",
    "    ('tfid', TfidfTransformer()),\n",
    "    (\"clf\", clf)\n",
    "    ])\n",
    "pipe_Tfidf.fit(X_train, Y_train)\n",
    "preds = pipe_Tfidf.predict(X_test)\n",
    "accu = preds == Y_test\n",
    "print(\"accuracy: for Tfidf model is:\", accu.mean())\n",
    "# print most informative words with highest coeff for Not Diaster and Diaster\n",
    "printNMostInformative(vectorizer, clf, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other non-vectorised Classifiers\n",
    "1) Random Forest\n",
    "\n",
    "2) Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: for Random Forest model is: 0.7695980899323518\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "RF_clf = RandomForestClassifier(n_estimators=10)\n",
    "pipe_RF = Pipeline(\n",
    "    [(\"vect\", vectorizer),\n",
    "    (\"clf\", RF_clf)\n",
    "    ])\n",
    "pipe_RF.fit(X_train, Y_train)\n",
    "preds = pipe_RF.predict(X_test)\n",
    "accu = preds == Y_test\n",
    "print(\"accuracy: for Random Forest model is:\", accu.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: for Bayes model is: 0.7871070433744528\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Classifier\n",
    "pipe_bayes = Pipeline(\n",
    "    [(\"vect\", vectorizer),\n",
    "    ('bayes', MultinomialNB())\n",
    "    ])\n",
    "pipe_bayes.fit(X_train, Y_train)\n",
    "preds = pipe_bayes.predict(X_test)\n",
    "accu = preds == Y_test\n",
    "print(\"accuracy: for Bayes model is:\", accu.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy CNN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import random\n",
    "optimizer = nlp.begin_training()\n",
    "for itn in range(5):\n",
    "    random.shuffle(X_train)\n",
    "    for raw_text, entity_offsets in X_train:\n",
    "        doc = nlp.make_doc(raw_text)\n",
    "        gold = GoldParse(doc, entities=entity_offsets)\n",
    "        nlp.update([doc], [gold], drop=0.5, sgd=optimizer)\n",
    "nlp.to_disk(\"/model\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## This is a good schematic pipeline which hopefully I will do another version to get more weighted features\n",
    "\n",
    "```python\n",
    "model = Pipeline([\n",
    "    ('text_union', FeatureUnion(\n",
    "        transformer_list = [\n",
    "            ('entity_feature', Pipeline([\n",
    "                ('entity_extractor', EntityExtractor()),\n",
    "                ('entity_vect', CountVectorizer()),\n",
    "            ])),\n",
    "            ('keyphrase_feature', Pipeline([\n",
    "                ('keyphrase_extractor', KeyphraseExtractor()),\n",
    "                ('keyphrase_vect', TfidfVectorizer()),\n",
    "            ])),\n",
    "        ],\n",
    "        transformer_weights= {\n",
    "            'entity_feature': 0.6,\n",
    "            'keyphrase_feature': 0.2,\n",
    "        }\n",
    "    )),\n",
    "    ('clf', LogisticRegression()),\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object <genexpr> at 0x7fbf43cda450>\n"
     ]
    }
   ],
   "source": [
    "a = list(range(0,100,2))\n",
    "print(type(*l) for l in a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
