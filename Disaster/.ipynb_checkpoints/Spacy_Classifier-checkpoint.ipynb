{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-83ec14d1bff2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTOP_WORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# transformers\n",
    "from sklearn.base import TransformerMixin,BaseEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# statistical models\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy to preprocess text into lemmatised tokens\n",
    "### Sklearn pipeline models:\n",
    "    1) Countvectoriser\n",
    "    2) Tfidf Vectoriser\n",
    "    3) Random Forest\n",
    "    4) Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install spacy && python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= pd.read_csv('train.csv')\n",
    "df2 = pd.read_csv('test.csv')\n",
    "\n",
    "boot = False # resample the data to 10000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all the words into the same columns\n",
    "# not used here\n",
    "def concat_words(data):\n",
    "    keyword_filled = data.keyword.fillna('')\n",
    "    location_filled = data.location.fillna('')\n",
    "    data['all_words'] = data.text + ' ' + keyword_filled + ' ' + location_filled\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy\n",
    "df_train = concat_words(df1)\n",
    "df_test = concat_words(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_train, test_size=0.1, random_state=42)\n",
    "if boot == True: train = train.sample(n=10000, replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613 6851 762\n"
     ]
    }
   ],
   "source": [
    "print(len(df1), len(train), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to clean words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate nlp object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# define which pos to filter out\n",
    "noisy_pos_tags = ['PROP','NUM', 'SYM']\n",
    "disable_list = [\"ner\", \"parser\"]\n",
    "\n",
    "# functions to filter out noises and clean words\n",
    "# so fucking ugly please improve your coding skill!\n",
    "def isNoise(token):     \n",
    "    is_noise = False\n",
    "    if token.pos_ in noisy_pos_tags:\n",
    "        is_noise = True \n",
    "        #print('pos')\n",
    "    elif token.is_stop == True:\n",
    "        is_noise = True\n",
    "        #print('stop')\n",
    "    elif token.is_punct == True:\n",
    "        is_noise = True\n",
    "        #print('punct')\n",
    "    elif token.lemma_ == '...':\n",
    "        is_noise = True\n",
    "        #print('...')\n",
    "    elif token.like_url == True:\n",
    "        is_noise = True\n",
    "        #print('url')\n",
    "    else:\n",
    "        is_noise = False\n",
    "    return is_noise \n",
    "\n",
    "# function in the tokenizer\n",
    "def cleanup(token, lower = True):\n",
    "    if lower:\n",
    "        token = token.lemma_.lower()\n",
    "    return token.strip()\n",
    "\n",
    "# tokentize words\n",
    "def lemma_tokenizer(sentence):\n",
    "    doc = nlp(sentence, disable_list)\n",
    "    tokens = [cleanup(word) for word in doc if isNoise(word) != True]\n",
    "    # return preprocessed list of tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These functions are for the pipeline for logistic ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dense transformer\n",
    "class ToDenseTransformer(BaseEstimator,TransformerMixin):\n",
    "    # define the transform operation\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    # no paramter to learn this case\n",
    "    # fit just returns an unchanged object\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printNMostInformative(vectorizer, clf, N):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    topClass1 = coefs_with_fns[:N]\n",
    "    topClass2 = coefs_with_fns[:-(N + 1):-1] # reverse the order\n",
    "    print(\"Not Disaster Best Words: \")\n",
    "    for feat in topClass1:\n",
    "        print(feat)\n",
    "    print(\"Disaster Best words: \")\n",
    "    for feat in topClass2:\n",
    "        print(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Training and Verification Data\n",
    "X_train = train['all_words'].tolist()\n",
    "Y_train = train['target'].tolist()\n",
    "X_test = test['all_words'].tolist()\n",
    "Y_test = test['target'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate vectorizer and pipeline object\n",
    "# Bag of words vectorizer\n",
    "vectorizer = CountVectorizer(tokenizer=lemma_tokenizer)\n",
    "clf = LogisticRegression(solver='liblinear', C=1.0) \n",
    "\n",
    "# all objects in the pipeline should be transformers\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer),\n",
    "    ('normal', ToDenseTransformer()),\n",
    "    (\"clf\", clf)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: for countvectorizer model on dataset is: 0.7913385826771654\n",
      "Not Disaster Best Words: \n",
      "(-1.2090964196530114, 'nowplaye')\n",
      "(-1.2015768048192368, 'bitch')\n",
      "(-1.1769614375324942, 'drink')\n",
      "(-1.1745596507699239, 'love')\n",
      "(-1.1689844530177425, 'cake')\n",
      "(-1.1517056529340135, 'ebay')\n",
      "(-1.146022153235248, 'blight')\n",
      "(-1.1310463862256472, 'soul')\n",
      "(-1.1226434941261827, 'likely')\n",
      "(-1.1146303870108922, 'write')\n",
      "Disaster Best words: \n",
      "(2.5137432106463407, 'hiroshima')\n",
      "(2.0484842524913622, 'wildfire')\n",
      "(1.8219031570221906, 'debris')\n",
      "(1.8102836651714251, 'typhoon')\n",
      "(1.7581133959435058, 'derailment')\n",
      "(1.647875729228661, 'suicide')\n",
      "(1.6410680942361067, 'earthquake')\n",
      "(1.5716588680653525, 'bombing')\n",
      "(1.5039706826270332, 'migrant')\n",
      "(1.495766802922914, 'spill')\n"
     ]
    }
   ],
   "source": [
    "# Specify logistic regression\n",
    "# Ridge Regression\n",
    "# C is the inverse of 'strength' lambda parameter of regularization (ie penalise large parameters), \n",
    "# in here smaller positive number greater strength)\n",
    "# Scikitlearn logistic regression apparently cannot set custom decision threshold (like WTF why)\n",
    "\n",
    "# train\n",
    "pipe.fit(X_train, Y_train)\n",
    "# test\n",
    "preds = pipe.predict(X_test)\n",
    "accu = preds == Y_test\n",
    "print(\"accuracy: for countvectorizer model on dataset is:\", accu.mean())\n",
    "printNMostInformative(vectorizer, clf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7854641  0.79640981 0.78580815]\n"
     ]
    }
   ],
   "source": [
    "# run cross-validation analyses on training set\n",
    "scores = cross_val_score(pipe, X_train,Y_train,cv=3)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: for Tfidf model is: 0.7874015748031497\n",
      "Not Disaster Best Words: \n",
      "(-2.3557055833766563, 'panic')\n",
      "(-2.3088845649647327, 'love')\n",
      "(-1.9242884729066396, 'scream')\n",
      "(-1.8667548786696497, 'blaze')\n",
      "(-1.7937751556735289, 'want')\n",
      "(-1.779170249434287, 'blight')\n",
      "(-1.7710914546222467, 'let')\n",
      "(-1.7541406528320656, 'aftershock')\n",
      "(-1.686378810936804, 'ruin')\n",
      "(-1.5804894152340216, 'armageddon')\n",
      "Disaster Best words: \n",
      "(3.4583022753835087, 'hiroshima')\n",
      "(3.207333168541642, 'kill')\n",
      "(3.0035460292894247, 'wildfire')\n",
      "(2.737594751283122, 'suicide')\n",
      "(2.6942904411593784, 'typhoon')\n",
      "(2.6268413034107376, 'bombing')\n",
      "(2.5875632553075345, 'debris')\n",
      "(2.551554724896847, 'derailment')\n",
      "(2.5360104046771497, 'building')\n",
      "(2.511480770222869, 'train')\n"
     ]
    }
   ],
   "source": [
    "# Tfidf vectorizer\n",
    "pipe_Tfidf = Pipeline(\n",
    "    [(\"vect\", vectorizer),\n",
    "    ('tfid', TfidfTransformer()),\n",
    "    (\"clf\", clf)\n",
    "    ])\n",
    "scores = cross_val_score(pipe, X_train,Y_train,cv=3)\n",
    "\n",
    "pipe_Tfidf.fit(X_train, Y_train)\n",
    "preds = pipe_Tfidf.predict(X_test)\n",
    "accu = preds == Y_test\n",
    "print(\"accuracy: for Tfidf model is:\", accu.mean())\n",
    "# print most informative words with highest coeff for Not Diaster and Diaster\n",
    "printNMostInformative(vectorizer, clf, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Classifiers\n",
    "1) Random Forest\n",
    "\n",
    "2) Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: for Random Forest model is: 0.7611548556430446\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "RF_clf = RandomForestClassifier(n_estimators=10)\n",
    "pipe_RF = Pipeline(\n",
    "    [(\"vect\", vectorizer),\n",
    "    (\"clf\", RF_clf)\n",
    "    ])\n",
    "pipe_RF.fit(X_train, Y_train)\n",
    "preds = pipe_RF.predict(X_test)\n",
    "accu = preds == Y_test\n",
    "print(\"accuracy: for Random Forest model is:\", accu.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: for Bayes model is: 0.7795275590551181\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Classifier\n",
    "pipe_bayes = Pipeline(\n",
    "    [(\"vect\", vectorizer),\n",
    "    ('bayes', MultinomialNB())\n",
    "    ])\n",
    "pipe_bayes.fit(X_train, Y_train)\n",
    "preds = pipe_bayes.predict(X_test)\n",
    "accu = preds == Y_test\n",
    "print(\"accuracy: for Bayes model is:\", accu.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just for fun to see the sparse NLP feature can be effective at clustering \n",
    "### Unsupervised k-means with cluster = 2 (as there are only 2 known categiries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: for clustering model is 0.4343891402714932 or 0.5656108597285068       (unsure since the kmean label was generated by the model)\n"
     ]
    }
   ],
   "source": [
    "kmean_clf = KMeans(n_clusters=2, random_state=0)\n",
    "pipe_kmean = Pipeline(\n",
    "    [(\"vect\", vectorizer),\n",
    "    (\"clf\", kmean_clf),    \n",
    "    ])\n",
    "\n",
    "pipe_kmean.fit(X_train)\n",
    "pred = pipe_kmean.predict(X_train) # Output cluster labels\n",
    "accu = np.equal(pred, np.array(Y_train))\n",
    "print(\"accuracy: for clustering model is {0} or {1} \\\n",
    "      (unsure since the kmean label was generated by the model)\".format( accu.mean(), 1- accu.mean()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f1621762f656>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe_kmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rainbow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "dist = pipe_kmean.transform(X_train)\n",
    "\n",
    "plt.scatter(dist[:,0], dist[:,1], c=pred, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## This is a good schematic pipeline which hopefully I will do another version to get more weighted features\n",
    "\n",
    "```python\n",
    "model = Pipeline([\n",
    "    ('text_union', FeatureUnion(\n",
    "        transformer_list = [\n",
    "            ('entity_feature', Pipeline([\n",
    "                ('entity_extractor', EntityExtractor()),\n",
    "                ('entity_vect', CountVectorizer()),\n",
    "            ])),\n",
    "            ('keyphrase_feature', Pipeline([\n",
    "                ('keyphrase_extractor', KeyphraseExtractor()),\n",
    "                ('keyphrase_vect', TfidfVectorizer()),\n",
    "            ])),\n",
    "        ],\n",
    "        transformer_weights= {\n",
    "            'entity_feature': 0.6,\n",
    "            'keyphrase_feature': 0.2,\n",
    "        }\n",
    "    )),\n",
    "    ('clf', LogisticRegression()),\n",
    "])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
