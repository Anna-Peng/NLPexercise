{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/S2DSLondon/Aug20_FSA/blob/Anna-wip/NLP%20practice-Anna/NLTK_Classifier.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer\n",
    "from nltk.tag import pos_tag\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk import classify\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The script is seperated into two parts\n",
    "\n",
    "### 1) Train the Model\n",
    "\n",
    "Setting:\n",
    "\n",
    "    Divide train-test set\n",
    "    Whether to save the classifier or not\n",
    "    \n",
    "Preprocessing\n",
    "\n",
    "    Read training data\n",
    "    Checking how many NaN\n",
    "    Combine all words (location, keyword, text) into one column\n",
    "    Seperate disaster and not disaster dataset\n",
    "    Make positive/negative token lists using tweet tokenizer\n",
    "    Lemmetize sentences\n",
    "    Extracting the most frequent words for sentiment analyses\n",
    "    Train Model\n",
    "    Confusion Matrix\n",
    "    Optional: Save the classifier for future use\n",
    "    \n",
    "### 2) Predict unlabeled data on the trained classifier above (or load existing classifier)\n",
    "\n",
    "preprocess data\n",
    "    \n",
    "    classify\n",
    "    save result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Train The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whether to save the classifier or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_classifier = True\n",
    "filename = 'Naive_Bayes_NLTK_v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training data\n",
    "Create a seperate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change this to the train file you want to read\n",
    "id_name = 'train.csv'\n",
    "raw_data = pd.read_csv(id_name)\n",
    "data = raw_data\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking how many NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 2533\n"
     ]
    }
   ],
   "source": [
    "print(sum(pd.isnull(data.keyword)), sum(pd.isnull(data.location)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all words (location, keyword, text) into one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_filled = data.keyword.fillna('')\n",
    "location_filled = data.location.fillna('')\n",
    "data['all_words'] = data.text + ' ' + keyword_filled + ' ' + location_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate disaster and not disaster dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disaster:  3271 No Disaster:  4342\n"
     ]
    }
   ],
   "source": [
    "rating = pd.to_numeric(data['target'])\n",
    "positive = data['all_words'][rating ==1] # Disaster = 1\n",
    "negative = data['all_words'][rating ==0] # \n",
    "print('Disaster: ', len(positive), 'No Disaster: ', len(negative))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make positive/negative token lists using tweet tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tokens = list()\n",
    "negative_tokens = list()\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "#tokenizer = RegexpTokenizer(r'\\w+') # regular expression to take out the symbols\n",
    "\n",
    "for pos_sentence  in positive:\n",
    "    add_pos_sentence = tweet_tokenizer.tokenize(pos_sentence)\n",
    "    positive_tokens.append(add_pos_sentence)\n",
    "    \n",
    "for neg_sentence in negative:\n",
    "    add_neg_sentence = tweet_tokenizer.tokenize(neg_sentence)\n",
    "    negative_tokens.append(add_neg_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmetize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add additional words to stop words\n",
    "stop_words = stopwords.words('english')\n",
    "add_words = [\"...\", \"'\"]\n",
    "stop_words = stop_words + add_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to lemmitize and clean words\n",
    "def cleaned_words (tokens, stop_words): # lemmatize sentence, omit punctuation and stop words\n",
    "    cleaned_tokens = []\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else: # all the rest tagged with a\n",
    "            pos = 'a'\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        word = lemmatizer.lemmatize(token, pos)\n",
    "        \n",
    "        if len(word) > 0 and word not in string.punctuation and word.lower() not in stop_words:\n",
    "            cleaned_tokens.append(word.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_cleaned_tokens_list=list()\n",
    "negative_cleaned_tokens_list=list()\n",
    "for tokens in positive_tokens:\n",
    "    positive_cleaned_tokens_list.append(cleaned_words(tokens, stop_words))\n",
    "\n",
    "for tokens in negative_tokens:\n",
    "    negative_cleaned_tokens_list.append(cleaned_words(tokens, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"what's\", 'man'], ['love', 'fruit'], ['summer', 'lovely'], ['car', 'fast'], ['goooooooaaaaaal'], ['ridiculous'], ['london', 'cool', ';)'], ['love', 'skiing'], ['wonderful', 'day'], ['looooool']]\n"
     ]
    }
   ],
   "source": [
    "print(negative_cleaned_tokens_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the most frequent words for own understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\\x89', 383), ('fire', 291), ('suicide', 204), ('รป_', 171), ('bomb', 164), ('california', 147), ('crash', 143), ('flood', 142), ('kill', 141), ('building', 137)]\n",
      "[('\\x89', 442), ('new', 301), ('get', 298), ('like', 295), ('body', 216), (\"i'm\", 207), ('go', 190), ('รป_', 171), ('scream', 152), ('wreck', 141)]\n"
     ]
    }
   ],
   "source": [
    "# Get word frequecy\n",
    "\n",
    "def get_all_words(cleaned_tokens_list): \n",
    "    all_words = []\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            all_words.append(token)\n",
    "    return all_words\n",
    "\n",
    "all_neg_words = get_all_words(negative_cleaned_tokens_list)\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "freq_dist_neg = FreqDist(all_neg_words)\n",
    "\n",
    "print(freq_dist_pos.most_common(10))\n",
    "print(freq_dist_neg.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce dictionary (in generator) for NLTK Naive Bayes Classifier, as it only takes dictionary\n",
    "\n",
    "def get_dict_for_model(cleaned_tokens_list):\n",
    "    for list_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in list_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_dict_for_model(positive_cleaned_tokens_list) \n",
    "negative_tokens_for_model = get_dict_for_model(negative_cleaned_tokens_list) \n",
    "# Convert generator into list\n",
    "pos_dataset = [(dict_word, 1) \n",
    "               for dict_word in positive_tokens_for_model]\n",
    "neg_dataset = [(dict_word, 0) \n",
    "               for dict_word in negative_tokens_for_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the dataset, and seperate into train and test(verification) set\n",
    "all_set = pos_dataset + neg_dataset\n",
    "random.shuffle(all_set)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(all_set, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n",
      "Accuracy is: 0.7711898129725427\n",
      "Most Informative Features\n",
      "               hiroshima = True                1 : 0      =     38.9 : 1.0\n",
      "                 typhoon = True                1 : 0      =     29.1 : 1.0\n",
      "                wildfire = True                1 : 0      =     28.7 : 1.0\n",
      "                      70 = True                1 : 0      =     27.3 : 1.0\n",
      "                outbreak = True                1 : 0      =     23.7 : 1.0\n",
      "                   spill = True                1 : 0      =     23.7 : 1.0\n",
      "                      40 = True                1 : 0      =     21.9 : 1.0\n",
      "                 warning = True                1 : 0      =     21.9 : 1.0\n",
      "                 20spill = True                1 : 0      =     21.9 : 1.0\n",
      "                 suicide = True                1 : 0      =     21.0 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "print(classifier.labels())\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_set))\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |      0      1 |\n",
      "--+---------------+\n",
      "0 | <43.7%> 12.7% |\n",
      "1 |  10.1% <33.4%>|\n",
      "--+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import ConfusionMatrix\n",
    "test_tag = [tag[0] for tag in test_set]\n",
    "test_label = [sent[1] for sent in test_set]\n",
    "model_label = classifier.classify_many(test_tag)\n",
    "cm = ConfusionMatrix(test_label, model_label)\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Save the classifier for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_classifier == True:\n",
    "    import pickle\n",
    "    seperator = ''\n",
    "    filename = seperator.join([filename, '.pickle'])\n",
    "    f = open(filename, 'wb')\n",
    "    pickle.dump(classifier, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Predict unlabeled data on the trained classifier above (or load existing classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_classifier(classifier):\n",
    "    import pickle\n",
    "    f = open(classifier, 'rb')\n",
    "    classifier = pickle.load(f)\n",
    "    return classifier\n",
    "    #f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the review file you want to read\n",
    "#classifier_id = 'Naive_Bayes_Classifier'\n",
    "id_name = 'test.csv'\n",
    "test_data = pd.read_csv(id_name)\n",
    "#import_classifier(classifier_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess unlabeled test data the same way as we did for the training data\n",
    "1) combine all words\n",
    "\n",
    "2) get tokenized words\n",
    "\n",
    "3) make dictionary (without label this time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_filled = test_data.keyword.fillna('')\n",
    "location_filled = test_data.location.fillna('')\n",
    "test_data['all_words'] = test_data.text + ' ' + keyword_filled + ' ' + location_filled\n",
    "test_token=list()\n",
    "cleaned_tokens_list = list()\n",
    "\n",
    "for sent in test_data['all_words']:\n",
    "    add_sent = tweet_tokenizer.tokenize(sent)\n",
    "    test_token.append(add_sent)\n",
    "for tokens in test_token:\n",
    "    cleaned_tokens_list.append(cleaned_words(tokens, stop_words))\n",
    "\n",
    "tokens_for_model = get_dict_for_model(cleaned_tokens_list) \n",
    "test_set = [dict_word for dict_word in tokens_for_model]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply classifier to the unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tag = [tag for tag in test_set]\n",
    "result = pd.DataFrame({'id': test_data.id})\n",
    "result['target'] = classifier.classify_many(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('result_NLTK.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
