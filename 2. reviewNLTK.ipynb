{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the review file you want to read\n",
    "id_name = 'The Prince of Wales, London.csv'\n",
    "\n",
    "data = pd.read_csv(id_name)\n",
    "filename = data['Restaurant_name'][0]\n",
    "divide_set = 3 # proportion of test set = 1/divide_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you want to use an existing classifier to read new review file, load the classifier here\n",
    "## If you want to save a new classifier, chage save_classifier to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_id = ''\n",
    "save_classifier = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Good and Bad reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = pd.to_numeric(data['review_rating'])\n",
    "positive_reviews = data['review_content'][rating > 3] # 4* and 5* are classed as a good review\n",
    "negative_reviews = data['review_content'][rating <= 3] # otherwise bad review\n",
    "print('Positive: ', len(positive_reviews), 'Negative: ', len(negative_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import tokenizer style and 'Part of Speech' taggig package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make positive/negative token lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tokens = list()\n",
    "negative_tokens = list()\n",
    "\n",
    "#tokenizer = RegexpTokenizer(r'\\w+') # regular expression to take out the symbols\n",
    "\n",
    "for pos_sentence  in positive_reviews:\n",
    "    add_pos_sentence = word_tokenize(pos_sentence)\n",
    "    positive_tokens.append(add_pos_sentence)\n",
    "    \n",
    "for neg_sentence in negative_reviews:\n",
    "    add_neg_sentence = word_tokenize(neg_sentence)\n",
    "    negative_tokens.append(add_neg_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the length of tokens still match with the no. of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Positive Tokens: ', len(positive_tokens), 'Negative Tokens: ', len(negative_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print some of Part-of-Speech in tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos_tag(positive_tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import stemming or lemmatizer, Stopwords, regular expression and special strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "add_words = [\"...\", \"'\"]\n",
    "stop_words = stop_words + add_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of lemmatizer and stemmer\n",
    "``` python\n",
    "lemmitizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = ['good', 'best', 'excellent', 'better', 'swim', 'swam']\n",
    "for w in words:\n",
    "    print(w, ':', lemmitizer.lemmatize(w,'a'))\n",
    "    print(w, ':', stemmer.stem(w))\n",
    "    print('--------')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_words (tokens, stop_words): # lemmatize sentence, omit punctuation and stop words such as preposition\n",
    "    cleaned_tokens = []\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else: # all the rest tagged with a\n",
    "            pos = 'a'\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        word = lemmatizer.lemmatize(token, pos)\n",
    "        \n",
    "        if len(word) > 0 and word not in string.punctuation and word.lower() not in stop_words:\n",
    "            cleaned_tokens.append(word.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_cleaned_tokens_list=list()\n",
    "negative_cleaned_tokens_list=list()\n",
    "for tokens in positive_tokens:\n",
    "    positive_cleaned_tokens_list.append(cleaned_words(tokens, stop_words))\n",
    "\n",
    "for tokens in negative_tokens:\n",
    "    negative_cleaned_tokens_list.append(cleaned_words(tokens, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(negative_cleaned_tokens_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the most frequent words for sentiment analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word frequecy\n",
    "\n",
    "def get_all_words(cleaned_tokens_list): \n",
    "    all_words = []\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            all_words.append(token)\n",
    "    return all_words\n",
    "\n",
    "all_neg_words = get_all_words(negative_cleaned_tokens_list)\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "freq_dist_neg = FreqDist(all_neg_words)\n",
    "main_pos_words = freq_dist_pos.most_common(30)\n",
    "main_neg_words = freq_dist_neg.most_common(30)\n",
    "\n",
    "print(freq_dist_pos.most_common(30))\n",
    "print(freq_dist_neg.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_for_model(cleaned_tokens_list):\n",
    "    for list_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in list_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_dict_for_model(positive_cleaned_tokens_list) \n",
    "negative_tokens_for_model = get_dict_for_model(negative_cleaned_tokens_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for neg_dict in negative_tokens_for_model:  \n",
    "    print(neg_dict)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dataset = [(dict_word, \"Positive\") \n",
    "               for dict_word in positive_tokens_for_model]\n",
    "neg_dataset = [(dict_word, \"Negative\") \n",
    "               for dict_word in negative_tokens_for_model]\n",
    "\n",
    "random.shuffle(neg_dataset)\n",
    "random.shuffle(pos_dataset)\n",
    "\n",
    "positive_dataset = pos_dataset[0:round(len(pos_dataset)*(1-1/divide_set))]\n",
    "negative_dataset = neg_dataset[0:round(len(neg_dataset)*(1-1/divide_set))]\n",
    "\n",
    "test_set = pos_dataset[round(len(pos_dataset)*(1-1/divide_set)): len(pos_dataset)] + neg_dataset[round(len(neg_dataset)*(1-1/divide_set)): len(neg_dataset)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Positive Training Data:', len(positive_dataset), 'Negative Training Data:', len(negative_dataset),\n",
    "     'Test_dataset:', len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_train = []\n",
    "negative_train = []\n",
    "training_set = []\n",
    "\n",
    "positive_train = positive_dataset\n",
    "negative_train = negative_dataset\n",
    "training_set = positive_train + negative_train\n",
    "random.shuffle(training_set)\n",
    "random.shuffle(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatively this code trains equal number of positive sentiment and negative sentiment by sampling with replacement\n",
    "\n",
    "```python\n",
    "\n",
    "training_no = 5000\n",
    "index1 = np.random.choice(len(positive_dataset), training_no)\n",
    "index2 = np.random.choice(len(negative_dataset), training_no)\n",
    "\n",
    "positive_train =[positive_dataset[i] for i in index1]\n",
    "negative_train =[negative_dataset[i] for i in index2]\n",
    "training_set = positive_train + negative_train\n",
    "random.shuffle(training_set)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_classifier(classifier):\n",
    "    import pickle\n",
    "    f = open(classifier, 'rb')\n",
    "    classifier = pickle.load(f)\n",
    "    return classifier\n",
    "    #f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "from nltk import classify\n",
    "classifier = NaiveBayesClassifier.train(training_set) if classifier_id =='' else import_classifier(classifier_id)\n",
    "print(classifier.labels())\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_set))\n",
    "print(classifier.show_most_informative_features(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics import ConfusionMatrix\n",
    "test_tag = [tag[0] for tag in test_set]\n",
    "test_label = [sent[1] for sent in test_set]\n",
    "model_label = classifier.classify_many(test_tag)\n",
    "cm = ConfusionMatrix(test_label, model_label)\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_classifier == True:\n",
    "    import pickle\n",
    "    seperator = ''\n",
    "    filename = seperator.join([filename, '.pickle'])\n",
    "    f = open(filename, 'wb')\n",
    "    pickle.dump(classifier, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### just randomly exploring how to use ngram. irrelevant to the exercise\n",
    "\n",
    "```python\n",
    "from nltk import ngrams\n",
    "\n",
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "\n",
    "n = 6\n",
    "sixgrams = ngrams(sentence.split(), n)\n",
    "\n",
    "for grams in sixgrams:\n",
    "    print(grams)\n",
    "    \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
