{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "#training_no = 5000 # iterative training randomly selected from training set\n",
    "data = pd.read_csv('The Golden Chippy.csv')\n",
    "filename = data['Restaurant_name'][0]\n",
    "divide_set = 3 # proportion of test set = 1/divide_set\n",
    "classifier_id = ''\n",
    "save_classifier = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Good and Bad reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive:  2600 Negative:  55\n"
     ]
    }
   ],
   "source": [
    "rating = pd.to_numeric(data['review_rating'])\n",
    "positive_reviews = data['review_title'][rating > 3]\n",
    "negative_reviews = data['review_title'][rating <= 3]\n",
    "print('Positive: ', len(positive_reviews), 'Negative: ', len(negative_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import tokenizer style and 'Part of Word' taggig package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make positive/negative token lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tokens = list()\n",
    "negative_tokens = list()\n",
    "#tokenizer = RegexpTokenizer(r'\\w+') # regular expression to take out the symbols\n",
    "\n",
    "for pos_sentence  in positive_reviews:\n",
    "    add_pos_sentence = word_tokenize(pos_sentence)\n",
    "    positive_tokens.append(add_pos_sentence)\n",
    "    \n",
    "for neg_sentence in negative_reviews:\n",
    "    add_neg_sentence = word_tokenize(neg_sentence)\n",
    "    negative_tokens.append(add_neg_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the length of tokens still match with the no. of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Tokens:  2600 Negative Tokens:  55\n"
     ]
    }
   ],
   "source": [
    "print('Positive Tokens: ', len(positive_tokens), 'Negative Tokens: ', len(negative_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech in tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(pos_tag(positive_tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import stemming or lemmatizer\n",
    "# Import Stopwords\n",
    "# import regular expression and special strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good : good\n",
      "good : good\n",
      "--------\n",
      "best : best\n",
      "best : best\n",
      "--------\n",
      "excellent : excellent\n",
      "excellent : excel\n",
      "--------\n",
      "better : good\n",
      "better : better\n",
      "--------\n",
      "swim : swim\n",
      "swim : swim\n",
      "--------\n",
      "swam : swam\n",
      "swam : swam\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "lemmitizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = ['good', 'best', 'excellent', 'better', 'swim', 'swam']\n",
    "for w in words:\n",
    "    print(w, ':', lemmitizer.lemmatize(w, 'a'))\n",
    "    print(w, ':', stemmer.stem(w))\n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_words (tokens, stop_words): # lemmatize sentence, omit punctuation and stop words such as preposition\n",
    "    cleaned_tokens = []\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else: # all the rest tagged with a\n",
    "            pos = 'a'\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        word = lemmatizer.lemmatize(token, pos)\n",
    "        \n",
    "        if len(word) > 0 and word not in string.punctuation and word.lower() not in stop_words:\n",
    "            cleaned_tokens.append(word.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_cleaned_tokens_list=list()\n",
    "negative_cleaned_tokens_list=list()\n",
    "for tokens in positive_tokens:\n",
    "    positive_cleaned_tokens_list.append(cleaned_words(tokens, stop_words))\n",
    "\n",
    "for tokens in negative_tokens:\n",
    "    negative_cleaned_tokens_list.append(cleaned_words(tokens, stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the most frequent words for sentiment analyses (or all words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fish', 'best', 'chip', 'chips', 'london', 'fresh', 'food', 'good', 'chippy', 'great', 'ever', 'amazing', '...', 'shop', 'n', 'tasty', 'one', 'life', 'birthday', 'cook', 'south', 'thank', 'amzing', 'halal', 'chris', 'variety', 'adam', 'catalina', 'greenwich/stamford', 'ct']\n",
      "['fish', 'chip', 'food', 'average', 'good', 'staff', 'bad', 'friendly', 'ok', 'chippy', 'nothing', 'review', 'rude', 'rat', 'star', '...', 'london', 'disappointed', 'great', 'fresh', 'special', 'nice', 'non', 'chicken', 'cooked', 'dry', 'many', 'shin', 'quite', 'spot']\n"
     ]
    }
   ],
   "source": [
    "# Get word frequecy\n",
    "\n",
    "def get_all_words(cleaned_tokens_list): \n",
    "    all_words = []\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            all_words.append(token)\n",
    "    return all_words\n",
    "\n",
    "all_neg_words = get_all_words(negative_cleaned_tokens_list)\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "freq_dist_neg = FreqDist(all_neg_words)\n",
    "main_pos_words = freq_dist_pos.most_common(30)\n",
    "main_neg_words = freq_dist_neg.most_common(30)\n",
    "\n",
    "#freq_pos_words = [main_pos_words[i][0] for i in range(0, len(main_pos_words))]\n",
    "#freq_neg_words = [main_neg_words[i][0] for i in range(0, len(main_neg_words))]\n",
    "\n",
    "print(freq_dist_pos.most_common(30))\n",
    "print(freq_dist_neg.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_for_model(cleaned_tokens_list):\n",
    "    for list_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in list_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_dict_for_model(positive_cleaned_tokens_list) \n",
    "negative_tokens_for_model = get_dict_for_model(negative_cleaned_tokens_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for neg_dict in negative_tokens_for_model:  \n",
    "    print(neg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dataset = [(dict_word, \"Positive\") \n",
    "               for dict_word in positive_tokens_for_model]\n",
    "neg_dataset = [(dict_word, \"Negative\") \n",
    "               for dict_word in negative_tokens_for_model]\n",
    "random.shuffle(neg_dataset)\n",
    "random.shuffle(pos_dataset)\n",
    "all_data = neg_dataset + pos_dataset\n",
    "positive_dataset = pos_dataset[0:round(len(pos_dataset)*(1-1/divide_set))]\n",
    "negative_dataset = neg_dataset[0:round(len(neg_dataset)*(1-1/divide_set))]\n",
    "test_dataset = pos_dataset[round(len(pos_dataset)*(1-1/divide_set)): len(pos_dataset)] + neg_dataset[round(len(neg_dataset)*(1-1/divide_set)): len(neg_dataset)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Training Data:  1733 Negative Training Data:  37 Test_dataset:  885\n"
     ]
    }
   ],
   "source": [
    "print('Positive Training Data: ', len(positive_dataset), 'Negative Training Data: ', len(negative_dataset),\n",
    "     'Test_dataset: ', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_train=[]\n",
    "negative_train=[]\n",
    "training_set = []\n",
    "\n",
    "#index1 = np.random.choice(len(positive_dataset), training_no)\n",
    "#index2 = np.random.choice(len(negative_dataset), training_no)\n",
    "\n",
    "positive_train = positive_dataset#[positive_dataset[i] for i in index1]\n",
    "negative_train = negative_dataset#[negative_dataset[i] for i in index2]\n",
    "training_set = positive_train + negative_train\n",
    "random.shuffle(training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_classifier(classifier):\n",
    "    import pickle\n",
    "    f = open(classifier, 'rb')\n",
    "    classifier = pickle.load(f)\n",
    "    return classifier\n",
    "    #f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9254237288135593\n",
      "Most Informative Features\n",
      "                 average = True           Negati : Positi =    136.9 : 1.0\n",
      "                     bag = True           Negati : Positi =     45.6 : 1.0\n",
      "                    term = True           Negati : Positi =     45.6 : 1.0\n",
      "                     bit = True           Negati : Positi =     45.6 : 1.0\n",
      "                    rest = True           Negati : Positi =     45.6 : 1.0\n",
      "                    many = True           Negati : Positi =     45.6 : 1.0\n",
      "                    know = True           Negati : Positi =     45.6 : 1.0\n",
      "                    deal = True           Negati : Positi =     45.6 : 1.0\n",
      "                 nothing = True           Negati : Positi =     45.6 : 1.0\n",
      "                  highly = True           Negati : Positi =     45.6 : 1.0\n",
      "            disappointed = True           Negati : Positi =     45.6 : 1.0\n",
      "                    long = True           Negati : Positi =     45.6 : 1.0\n",
      "                    time = True           Negati : Positi =     27.4 : 1.0\n",
      "                       5 = True           Negati : Positi =     27.4 : 1.0\n",
      "                  review = True           Negati : Positi =     25.4 : 1.0\n",
      "                     big = True           Negati : Positi =     19.6 : 1.0\n",
      "                    star = True           Negati : Positi =     19.6 : 1.0\n",
      "                    spot = True           Negati : Positi =     19.6 : 1.0\n",
      "             exceptional = True           Negati : Positi =     19.6 : 1.0\n",
      "                     try = True           Negati : Positi =     15.2 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "from nltk import classify\n",
    "classifier = NaiveBayesClassifier.train(training_set) if classifier_id =='' else import_classifier(classifier_id)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_dataset))\n",
    "print(classifier.show_most_informative_features(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "prob_classify() missing 1 required positional argument: 'featureset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-568eec0bf008>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNaiveBayesClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: prob_classify() missing 1 required positional argument: 'featureset'"
     ]
    }
   ],
   "source": [
    "NaiveBayesClassifier.prob_classify(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_classifier == True:\n",
    "    import pickle\n",
    "    seperator = ''\n",
    "    filename = seperator.join([filename, '.pickle'])\n",
    "    f = open(filename, 'wb')\n",
    "    pickle.dump(classifier, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "\n",
    "n = 6\n",
    "sixgrams = ngrams(sentence.split(), n)\n",
    "\n",
    "for grams in sixgrams:\n",
    "    print(grams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
