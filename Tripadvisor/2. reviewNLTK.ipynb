{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the review file you want to read\n",
    "id_name = 'The Prince of Wales, London.csv'\n",
    "\n",
    "data = pd.read_csv(id_name)\n",
    "filename = data['Restaurant_name'][0]\n",
    "divide_set = 3 # proportion of test set = 1/divide_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you want to use an existing classifier to read new review file, load the classifier here\n",
    "## If you want to save a new classifier, chage save_classifier to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_id = ''\n",
    "save_classifier = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Good and Bad reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive:  2283 Negative:  22\n"
     ]
    }
   ],
   "source": [
    "rating = pd.to_numeric(data['review_rating'])\n",
    "positive_reviews = data['review_content'][rating > 3] # 4* and 5* are classed as a good review\n",
    "negative_reviews = data['review_content'][rating <= 3] # otherwise bad review\n",
    "print('Positive: ', len(positive_reviews), 'Negative: ', len(negative_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import tokenizer style and 'Part of Speech' taggig package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/annapeng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/annapeng/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/annapeng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/annapeng/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.tag import pos_tag\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make positive/negative token lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tokens = list()\n",
    "negative_tokens = list()\n",
    "\n",
    "#tokenizer = RegexpTokenizer(r'\\w+') # regular expression to take out the symbols\n",
    "\n",
    "for pos_sentence  in positive_reviews:\n",
    "    add_pos_sentence = word_tokenize(pos_sentence)\n",
    "    positive_tokens.append(add_pos_sentence)\n",
    "    \n",
    "for neg_sentence in negative_reviews:\n",
    "    add_neg_sentence = word_tokenize(neg_sentence)\n",
    "    negative_tokens.append(add_neg_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the length of tokens still match with the no. of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Tokens:  2283 Negative Tokens:  22\n"
     ]
    }
   ],
   "source": [
    "print('Positive Tokens: ', len(positive_tokens), 'Negative Tokens: ', len(negative_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print some of Part-of-Speech in tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Friendly', 'JJ'), ('staff', 'NN'), (',', ','), ('great', 'JJ'), ('atmosphere', 'NN'), ('and', 'CC'), ('even', 'RB'), ('better', 'JJR'), ('food', 'NN'), ('and', 'CC'), ('drink', 'NN'), ('.', '.'), ('Massive', 'JJ'), ('Sunday', 'NNP'), ('roast', 'NN'), ('for', 'IN'), ('£12', 'NNP'), ('-', ':'), ('bargain', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag(positive_tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import stemming or lemmatizer, Stopwords, regular expression and special strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "add_words = [\"...\", \"'\"]\n",
    "stop_words = stop_words + add_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of lemmatizer and stemmer\n",
    "``` python\n",
    "lemmitizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = ['good', 'best', 'excellent', 'better', 'swim', 'swam']\n",
    "for w in words:\n",
    "    print(w, ':', lemmitizer.lemmatize(w,'a'))\n",
    "    print(w, ':', stemmer.stem(w))\n",
    "    print('--------')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_words (tokens, stop_words): # lemmatize sentence, omit punctuation and stop words such as preposition\n",
    "    cleaned_tokens = []\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else: # all the rest tagged with a\n",
    "            pos = 'a'\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        word = lemmatizer.lemmatize(token, pos)\n",
    "        \n",
    "        if len(word) > 0 and word not in string.punctuation and word.lower() not in stop_words:\n",
    "            cleaned_tokens.append(word.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_cleaned_tokens_list=list()\n",
    "negative_cleaned_tokens_list=list()\n",
    "for tokens in positive_tokens:\n",
    "    positive_cleaned_tokens_list.append(cleaned_words(tokens, stop_words))\n",
    "\n",
    "for tokens in negative_tokens:\n",
    "    negative_cleaned_tokens_list.append(cleaned_words(tokens, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['food', 'pretty', 'good', 'beer', 'awful', 'beer', 'taste', 'like', 'watered-down', 'even', \"n't\", 'manage', 'drink', 'pint'], ['walk', 'place', 'way', 'imperial', 'war', 'museum', 'look', 'good', 'outside', 'lot', 'food', 'tout', 'decent', 'look', 'menu', 'way', 'back', 'elephant', 'castle', 'tube', 'think'], ['food', 'pretty', 'good', 'beer', 'awful', 'beer', 'taste', 'like', 'watered-down', 'even', \"n't\", 'manage', 'drink', 'pint'], ['walk', 'place', 'way', 'imperial', 'war', 'museum', 'look', 'good', 'outside', 'lot', 'food', 'tout', 'decent', 'look', 'menu', 'way', 'back', 'elephant', 'castle', 'tube', 'think'], ['think', 'go', 'great', 'night', 'read', 'review', '’', 'lively', 'expect', 'sunday', '’', 'big', 'deal', 'scouted', 'outside', 'free', 'shot', 'advertise', 'staff'], ['food', 'good', 'place', 'homey', 'service', 'bit', 'spotty', 'great', 'want', 'drink', 'read', 'book'], ['food', 'good', 'price', 'high', 'london', 'lack', 'service', 'waited', 'long', 'someone', 'come', 'take', 'order', 'friendly'], ['visited', 'last', 'sunday', '3ish', 'go', 'roast', 'beef', 'sunday', 'roast', 'usual', 'resemble', 'cold', 'roast', 'beef', 'purchase', 'supermarket', 'place', 'gravy', 'beef', 'slice', 'slicing', 'machine', 'shape'], ['terrible', 'food', 'rude', 'staff', 'dirty', 'establishment', 'go', 'elsewhere', 'win', 'regret', 'could', 'go', 'unfortunately', 'negative'], ['kind', 'staff', 'food', 'awful', 'restaurant', 'close', 'imperial', 'war', 'museum', 'hour', 'museum', 'hungry', 'step', 'us', 'felt', 'bad', 'afterwards', 'wife', '’', 'even', 'eat', 'half']]\n"
     ]
    }
   ],
   "source": [
    "print(negative_cleaned_tokens_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the most frequent words for sentiment analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('food', 1851), ('good', 1519), ('great', 1257), ('pub', 1133), ('friendly', 1085), ('service', 807), ('place', 790), ('staff', 740), ('roast', 549), ('best', 539), ('dinner', 515), ('london', 440), ('atmosphere', 439), ('fish', 426), ('prince', 423), ('chip', 420), ('lunch', 419), ('wales', 412), ('recommend', 408), ('local', 399), ('drink', 391), ('family', 388), ('sunday', 387), ('well', 381), ('pie', 350), ('also', 350), ('amaze', 341), ('greet', 327), ('visit', 310), ('lovely', 270)]\n",
      "[('food', 19), ('good', 8), ('place', 8), ('museum', 7), ('go', 7), ('us', 7), ('drink', 6), ('look', 6), ('pub', 6), (\"n't\", 5), ('way', 5), ('sunday', 5), ('come', 5), ('beer', 4), ('like', 4), ('imperial', 4), ('war', 4), ('’', 4), ('order', 4), ('roast', 4), ('beef', 4), ('awful', 3), ('taste', 3), ('even', 3), ('outside', 3), ('back', 3), ('think', 3), ('great', 3), ('staff', 3), ('want', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Get word frequecy\n",
    "\n",
    "def get_all_words(cleaned_tokens_list): \n",
    "    all_words = []\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            all_words.append(token)\n",
    "    return all_words\n",
    "\n",
    "all_neg_words = get_all_words(negative_cleaned_tokens_list)\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "freq_dist_neg = FreqDist(all_neg_words)\n",
    "main_pos_words = freq_dist_pos.most_common(30)\n",
    "main_neg_words = freq_dist_neg.most_common(30)\n",
    "\n",
    "print(freq_dist_pos.most_common(30))\n",
    "print(freq_dist_neg.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_for_model(cleaned_tokens_list):\n",
    "    for list_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in list_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_dict_for_model(positive_cleaned_tokens_list) \n",
    "negative_tokens_for_model = get_dict_for_model(negative_cleaned_tokens_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for neg_dict in negative_tokens_for_model:  \n",
    "    print(neg_dict)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dataset = [(dict_word, \"Positive\") \n",
    "               for dict_word in positive_tokens_for_model]\n",
    "neg_dataset = [(dict_word, \"Negative\") \n",
    "               for dict_word in negative_tokens_for_model]\n",
    "\n",
    "random.shuffle(neg_dataset)\n",
    "random.shuffle(pos_dataset)\n",
    "\n",
    "positive_dataset = pos_dataset[0:round(len(pos_dataset)*(1-1/divide_set))]\n",
    "negative_dataset = neg_dataset[0:round(len(neg_dataset)*(1-1/divide_set))]\n",
    "\n",
    "test_set = pos_dataset[round(len(pos_dataset)*(1-1/divide_set)): len(pos_dataset)] + neg_dataset[round(len(neg_dataset)*(1-1/divide_set)): len(neg_dataset)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Training Data: 1522 Negative Training Data: 15 Test_dataset: 768\n"
     ]
    }
   ],
   "source": [
    "print('Positive Training Data:', len(positive_dataset), 'Negative Training Data:', len(negative_dataset),\n",
    "     'Test_dataset:', len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_train = []\n",
    "negative_train = []\n",
    "training_set = []\n",
    "\n",
    "positive_train = positive_dataset\n",
    "negative_train = negative_dataset\n",
    "training_set = positive_train + negative_train\n",
    "random.shuffle(training_set)\n",
    "random.shuffle(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatively this code trains equal number of positive sentiment and negative sentiment by sampling with replacement\n",
    "\n",
    "```python\n",
    "\n",
    "training_no = 5000\n",
    "index1 = np.random.choice(len(positive_dataset), training_no)\n",
    "index2 = np.random.choice(len(negative_dataset), training_no)\n",
    "\n",
    "positive_train =[positive_dataset[i] for i in index1]\n",
    "negative_train =[negative_dataset[i] for i in index2]\n",
    "training_set = positive_train + negative_train\n",
    "random.shuffle(training_set)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_classifier(classifier):\n",
    "    import pickle\n",
    "    f = open(classifier, 'rb')\n",
    "    classifier = pickle.load(f)\n",
    "    return classifier\n",
    "    #f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Positive', 'Negative']\n",
      "Accuracy is: 0.75390625\n",
      "Most Informative Features\n",
      "                 thought = True           Negati : Positi =     95.2 : 1.0\n",
      "                    onto = True           Negati : Positi =     95.2 : 1.0\n",
      "                purchase = True           Negati : Positi =     95.2 : 1.0\n",
      "                 kitchen = True           Negati : Positi =     95.2 : 1.0\n",
      "                   gravy = True           Negati : Positi =     95.2 : 1.0\n",
      "                 explore = True           Negati : Positi =     95.2 : 1.0\n",
      "               elsewhere = True           Negati : Positi =     95.2 : 1.0\n",
      "                     due = True           Negati : Positi =     95.2 : 1.0\n",
      "                   homey = True           Negati : Positi =     95.2 : 1.0\n",
      "           unfortunately = True           Negati : Positi =     95.2 : 1.0\n",
      "               direction = True           Negati : Positi =     95.2 : 1.0\n",
      "                thursday = True           Negati : Positi =     95.2 : 1.0\n",
      "                 someone = True           Negati : Positi =     95.2 : 1.0\n",
      "                 average = True           Negati : Positi =     68.0 : 1.0\n",
      "                    read = True           Negati : Positi =     68.0 : 1.0\n",
      "                  mother = True           Negati : Positi =     57.1 : 1.0\n",
      "                  forget = True           Negati : Positi =     57.1 : 1.0\n",
      "               advertise = True           Negati : Positi =     57.1 : 1.0\n",
      "                   leave = True           Negati : Positi =     57.1 : 1.0\n",
      "                   usual = True           Negati : Positi =     57.1 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "from nltk import classify\n",
    "classifier = NaiveBayesClassifier.train(training_set) if classifier_id =='' else import_classifier(classifier_id)\n",
    "print(classifier.labels())\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_set))\n",
    "print(classifier.show_most_informative_features(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         |      P      N |\n",
      "         |      o      e |\n",
      "         |      s      g |\n",
      "         |      i      a |\n",
      "         |      t      t |\n",
      "         |      i      i |\n",
      "         |      v      v |\n",
      "         |      e      e |\n",
      "---------+---------------+\n",
      "Positive | <74.5%> 24.6% |\n",
      "Negative |      .  <0.9%>|\n",
      "---------+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import ConfusionMatrix\n",
    "test_tag = [tag[0] for tag in test_set]\n",
    "test_label = [sent[1] for sent in test_set]\n",
    "model_label = classifier.classify_many(test_tag)\n",
    "cm = ConfusionMatrix(test_label, model_label)\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_classifier == True:\n",
    "    import pickle\n",
    "    seperator = ''\n",
    "    filename = seperator.join([filename, '.pickle'])\n",
    "    f = open(filename, 'wb')\n",
    "    pickle.dump(classifier, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### just randomly exploring how to use ngram. irrelevant to the exercise\n",
    "\n",
    "```python\n",
    "from nltk import ngrams\n",
    "\n",
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "\n",
    "n = 6\n",
    "sixgrams = ngrams(sentence.split(), n)\n",
    "\n",
    "for grams in sixgrams:\n",
    "    print(grams)\n",
    "    \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
